{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the MNIST dataset. Reads the training and testing files and create matrices.\n",
    "    :Expected return:\n",
    "    train_data:the matrix with the training data\n",
    "    test_data: the matrix with the data that will be used for testing\n",
    "    y_train: the matrix consisting of one \n",
    "                        hot vectors on each row(ground truth for training)\n",
    "    y_test: the matrix consisting of one\n",
    "                        hot vectors on each row(ground truth for testing)\n",
    "    \"\"\"\n",
    "    \n",
    "    #load the train files\n",
    "    df = None\n",
    "    \n",
    "    y_train = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/train%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_train.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    train_data = df.values\n",
    "    y_train = np.array( y_train )\n",
    "    \n",
    "    #load test files\n",
    "    df = None\n",
    "    \n",
    "    y_test = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/test%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        \n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_test.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    test_data = df.values\n",
    "    y_test = np.array( y_test )\n",
    "    \n",
    "    return train_data, test_data, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use by default ax=1, when the array is 2D\n",
    "#use ax=0 when the array is 1D\n",
    "def softmax( x, ax=1 ):\n",
    "\n",
    "    m = np.max( x, axis=ax, keepdims=True )#max per row\n",
    "    p = np.exp( x - m )\n",
    "    return ( p / np.sum(p,axis=ax,keepdims=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oneLayerMLP:\n",
    "    \n",
    "    def __init__(self, hiddenUnits , activationFunction):\n",
    "        self.M = hiddenUnits\n",
    "        self.h , self.hDeriv = self.getFunctions(activationFunction)\n",
    "    \n",
    "    def train(self,X,t,l,learningRate,batchSize,K,epochs):\n",
    "        self.X =  np.hstack(( np.ones((X.shape[0],1)) , X ))    #adding the bias unit\n",
    "        self.t = t\n",
    "        self.l = l\n",
    "        self.batchSize = batchSize\n",
    "        self.K = K\n",
    "#         self.w1 = np.ones((self.M,self.X.shape[1]))\n",
    "#         self.w2 = np.ones((K,self.M+1)) \n",
    "        self.w1 = np.random.uniform( -np.sqrt(2/self.M) ,np.sqrt(2/self.M)  , (self.M,self.X.shape[1])) \n",
    "        self.w2 = np.random.uniform(-np.sqrt(2/K)  ,np.sqrt(2/K)  , (K,self.M+1)) \n",
    "        self.epochs = epochs\n",
    "        self.costs = []\n",
    "        \n",
    "        batchesList = self.createMiniBatches()\n",
    "\n",
    "        for e in range(epochs):\n",
    "            epochCost = []\n",
    "            for batchX , batchT in batchesList:\n",
    "                a , dW1 , dW2 = self.SGD(batchX,batchT,learningRate)\n",
    "                self.w1 =self.w1 + learningRate*dW1\n",
    "                self.w2 = self.w2 + learningRate*dW2\n",
    "                epochCost.append(a)\n",
    "            self.costs.append( sum(epochCost)/self.batchSize )\n",
    "    \n",
    "    def SGD(self,batchX,batchT,learningRate):\n",
    "        z,y = self.predict(batchX,addBias = False)\n",
    "        Error = np.sum(np.log(y)*batchT) - (self.l/2) * ( np.sum( np.square( self.w1 ) ) -  np.sum( np.square( self.w2 ) ))    \n",
    "        loss = (batchT - y)\n",
    "        dW1 = ( loss.dot(self.w2[:,1:]) * self.hDeriv(batchX.dot(self.w1.T)) ).T.dot(batchX)  - self.l*self.w1\n",
    "        dW2 = loss.T.dot(z) - self.l*self.w2\n",
    "        \n",
    "        return Error , dW1 ,dW2\n",
    "    \n",
    "    def predict(self,batchX , addBias=True ):\n",
    "        if addBias:\n",
    "            batchX =  np.hstack(( np.ones((batchX.shape[0],1)) , batchX ))  \n",
    "\n",
    "        z = self.h( batchX.dot(self.w1.T))\n",
    "        z = np.hstack(( np.ones((z.shape[0],1)) , z ))    #adding the bias unit\n",
    "        y = softmax(z.dot(self.w2.T))\n",
    "\n",
    "        return z,y\n",
    "    \n",
    "    def getFunctions(self,activationFunction):\n",
    "        if activationFunction==\"softplus\":\n",
    "            return lambda x: np.log(1+np.exp(x)) , lambda x: 1/(1+np.exp(-x))\n",
    "        elif activationFunction==\"tanh\": \n",
    "            return lambda x: (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) , lambda x: 1 - ((np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)))**2        \n",
    "        elif activationFunction==\"cos\":\n",
    "            return lambda x: np.cos(x) , lambda x: -np.sin(x)       \n",
    "        else:\n",
    "            print(\"Wrong code\")\n",
    "            return lambda x: 0  \n",
    "        \n",
    "        \n",
    "    def createMiniBatches(self):\n",
    "        stacked = np.hstack((self.X , self.t ))\n",
    "#         np.random.shuffle(stacked)  \n",
    "        batchesList = []\n",
    "        i=0\n",
    "        while i<stacked.shape[0]:\n",
    "            if (i+self.batchSize<stacked.shape[0]):\n",
    "                batchesList.append(stacked[i:i+self.batchSize])\n",
    "            else:\n",
    "                batchesList.append( stacked[i:])\n",
    "                \n",
    "            i+=self.batchSize\n",
    "        \n",
    "        batchesList = [ [ batch[:,:-self.K] , batch[:,-self.K:] ]  for batch in batchesList  ]\n",
    "        return batchesList\n",
    "    \n",
    "    \n",
    "    def gradcheck_softmax(self):\n",
    "\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        _list = np.random.randint(X.shape[0], size=5)\n",
    "        x_sample = np.array(self.X[_list, 0:100])\n",
    "        t_sample = np.array(self.t[_list, 0:100])\n",
    "        Ew, dW1 , dW2 = self.SGD(x_sample, t_sample, self.l)\n",
    "        num_dW1 = np.zeros(dW1.shape)\n",
    "        num_dW2 = np.zeros(dW2.shape)\n",
    "        old_W1 = np.copy(self.w1)\n",
    "        old_W2 = np.copy(self.w2)\n",
    "\n",
    "        # Compute all numerical gradient estimates and store them in\n",
    "        # the matrix numericalGrad\n",
    "        for k in range(num_dW1.shape[0]):\n",
    "            for d in range(num_dW1.shape[1]):\n",
    "                #add epsilon to the w[k,d]\n",
    "                self.w1 = np.copy(old_W1)\n",
    "                self.w1[k, d] += epsilon\n",
    "                e_plus, _ , _ = self.SGD(x_sample, t_sample, self.l)\n",
    "\n",
    "                #subtract epsilon to the w[k,d]\n",
    "                self.w1 = np.copy(old_W1)\n",
    "                self.w1[k, d] -= epsilon\n",
    "                e_minus, _ , _ = self.SGD(x_sample, t_sample, self.l)\n",
    "\n",
    "                #approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "                num_dW1[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "                            \n",
    "        self.w1 = np.copy(old_W1)\n",
    "        for k in range(num_dW2.shape[0]):\n",
    "            for d in range(num_dW2.shape[1]):\n",
    "\n",
    "                #add epsilon to the w[k,d]\n",
    "                self.w2 = np.copy(old_W2)\n",
    "                self.w2[k, d] += epsilon\n",
    "                e_plus, _ , _ = self.SGD(x_sample, t_sample, self.l)\n",
    "\n",
    "                #subtract epsilon to the w[k,d]\n",
    "                self.w2 = np.copy(old_W2)\n",
    "                self.w2[k, d] -= epsilon\n",
    "                e_minus, _ , _ = self.SGD(x_sample, t_sample, self.l)\n",
    "\n",
    "                #approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "                num_dW2[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "            \n",
    "            self.w2 = np.copy(old_W2)\n",
    "        return ( dW1, num_dW1 , dW2 , num_dW2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.407357449065413]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = oneLayerMLP(10,\"cos\")\n",
    "X = np.array([[1,2,3],[4,5,6],[7,8,9],[1,2,3],[4,6,6],[7,8,9] ,[1,2,3],[4,7,6],[7,8,9]   ])\n",
    "t = np.array([[0,1],[0,1],[1,0],[0,1],[0,1],[1,0],[0,1],[0,1],[1,0],])\n",
    "p.train(X,t,0.1,0.001,3,2,1)\n",
    "p.costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = p.gradcheck_softmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()\n",
    "X_train = X_train.astype(float)/255\n",
    "X_test = X_test.astype(float)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = oneLayerMLP(200,\"tanh\")\n",
    "X = X_train\n",
    "t =y_train\n",
    "p.train(X,t,0.1,0.001,200,10,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-275.3349161568674,\n",
       " -165.9352843751199,\n",
       " -116.97552200783475,\n",
       " -95.1927983876926,\n",
       " -80.8136538581284,\n",
       " -76.10773852503677,\n",
       " -77.54116953075827,\n",
       " -76.6699208531895,\n",
       " -73.33126050422004,\n",
       " -69.96731453033057,\n",
       " -66.38705916053249,\n",
       " -62.315192534935974,\n",
       " -58.179950180719935,\n",
       " -54.387042077534026,\n",
       " -52.65625224196683,\n",
       " -50.2650763235407,\n",
       " -48.16099974899589,\n",
       " -46.479534997949465,\n",
       " -45.31242266874721,\n",
       " -43.37000618654126]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , pred = p.predict(X_test,addBias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3597"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean( np.argmax(pred,1) == np.argmax(y_test,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-238-ec6a204a4295>:12: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  plt.subplot( sqrt_n, sqrt_n, cnt ).axis('off')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x792 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot 5 random images from the training set\n",
    "n = 5\n",
    "sqrt_n = int( n**0.5 )\n",
    "samples = np.random.randint(X_train.shape[0], size=n)\n",
    "samples = [5000,5001,5002,5003]\n",
    "plt.figure( figsize=(11,11) )\n",
    "\n",
    "cnt = 0\n",
    "for i in samples:\n",
    "    cnt += 1\n",
    "    plt.subplot( sqrt_n, sqrt_n, cnt )\n",
    "    plt.subplot( sqrt_n, sqrt_n, cnt ).axis('off')\n",
    "    plt.imshow( X_test[i].reshape(28,28), cmap='gray'  )\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
