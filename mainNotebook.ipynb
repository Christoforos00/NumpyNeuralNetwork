{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dictionary = pickle.load(fo, encoding='bytes')\n",
    "    return dictionary\n",
    "\n",
    "def load_data(path , dataset):\n",
    "    if dataset==\"cifar\":\n",
    "        X_train = []\n",
    "        y_train = np.empty((0,10), int)\n",
    "        for i in range(1,6):\n",
    "            currentFile = unpickle(path+\"/cifar-10-batches-py/data_batch_\"+ str(i))\n",
    "            current_X = currentFile[b'data']\n",
    "            X_train.extend( current_X )\n",
    "            current_labels = np.array(currentFile[b'labels'])\n",
    "            current_y = np.squeeze(np.eye(10)[current_labels.reshape(-1)])\n",
    "            y_train = np.vstack((y_train, current_y))\n",
    "\n",
    "        testFile = unpickle(path+\"/cifar-10-batches-py/test_batch\")\n",
    "        X_test = testFile[b'data']\n",
    "        labels = np.array(testFile[b'labels'])\n",
    "        y_test = np.squeeze(np.eye(10)[labels.reshape(-1)])\n",
    "\n",
    "        return np.array(X_train)/255, np.array(X_test)/255, y_train , y_test\n",
    "\n",
    "    if dataset==\"mnist\":\n",
    "        df = None\n",
    "        y_train = []\n",
    "        for i in range( 10 ):\n",
    "            tmp = pd.read_csv( path + '/mnist/train%d.txt' % i, header=None, sep=\" \" )\n",
    "            hot_vector = [ 1 if j == i else 0 for j in range(10) ] \n",
    "            for j in range( tmp.shape[0] ):\n",
    "                y_train.append( hot_vector )   \n",
    "            if i == 0:\n",
    "                df = tmp\n",
    "            else:\n",
    "                df = pd.concat( [df, tmp] )\n",
    "        train_data = df.values\n",
    "        y_train = np.array( y_train )\n",
    "\n",
    "        df = None\n",
    "        y_test = []\n",
    "        for i in range( 10 ):\n",
    "            tmp = pd.read_csv( path +'/mnist/test%d.txt' % i, header=None, sep=\" \" )\n",
    "            hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "            for j in range( tmp.shape[0] ):\n",
    "                y_test.append( hot_vector )  \n",
    "            if i == 0:\n",
    "                df = tmp\n",
    "            else:\n",
    "                df = pd.concat( [df, tmp] )\n",
    "        test_data = df.values\n",
    "        y_test = np.array( y_test )\n",
    "\n",
    "        return train_data.astype(float)/255, test_data.astype(float)/255, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def softmax( x, ax=1 ):\n",
    "    m = np.max( x, axis=ax, keepdims=True )    #max per row\n",
    "    p = np.exp( x - m )\n",
    "    return ( p / np.sum(p,axis=ax,keepdims=True) )\n",
    "\n",
    "class oneLayerMLP:\n",
    "    \n",
    "    def __init__(self, hiddenUnits , activationFunction):\n",
    "        self.M = hiddenUnits\n",
    "        self.h , self.hDeriv = self.getFunctions(activationFunction)\n",
    "\n",
    "        \n",
    "    def getFunctions(self,activationName):                  #returns the activation, and it's derivative\n",
    "        if activationName==\"softplus\":\n",
    "           return lambda x: np.log(1+np.exp(x)) , lambda x: 1/(1+np.exp(-x))\n",
    "        elif activationName==\"tanh\": \n",
    "            return lambda x: np.tanh(x) , lambda x: 1-np.power(np.tanh(x),2)\n",
    "        elif activationName==\"cos\":\n",
    "            return lambda x: np.cos(x) , lambda x: -np.sin(x)\n",
    "        else:\n",
    "            print(\"Wrong code\")\n",
    "            return lambda x: 0  \n",
    "        \n",
    "# \n",
    "        \n",
    "    def forward_propagation(self ,w1, w2, x):\n",
    "        z = self.h(x.dot(w1.T) )  \n",
    "        z = np.hstack(( np.ones((z.shape[0],1)) , z ))        #append the bias unit\n",
    "        y = softmax( z.dot(w2.T) )\n",
    "        return z, y\n",
    "\n",
    "    def backward_propagation( self, x, t, z, y, w1, w2 , l):\n",
    "        cost = np.sum(np.log(y)*t) - (l/2) * ( np.sum( np.square( w1 ) ) +  np.sum( np.square( w2 ) ) )    \n",
    "        error = t-y\n",
    "        dW1 = ( error.dot(w2[:,1:]) * self.hDeriv( x.dot(w1.T)) ).T.dot(x) - l*w1\n",
    "        dW2 = error.T.dot(z) - l*w2\n",
    "        return cost, dW1, dW2   \n",
    "    \n",
    "    \n",
    "    def train(self , X_train, y_train, lRate , train_epochs, batchSize, l):\n",
    "        X_train = np.append(np.ones((X_train.shape[0], 1)), X_train, axis=1)        #append the bias unit\n",
    "        lRate = lRate / batchSize\n",
    "        self.K = y_train.shape[1]\n",
    "        self.D = X_train.shape[1]\n",
    "         \n",
    "        s1 = np.sqrt(2/self.D)                            #weight initialization\n",
    "        s2 = np.sqrt(2/self.M + 1)       \n",
    "        self.w1 = np.random.uniform(-s1 ,s1  , (self.M,self.D)) \n",
    "        self.w2 = np.random.uniform(-s2 ,s2  , (self.K,self.M+1))    \n",
    "        \n",
    "        for e in range(train_epochs):\n",
    "            for batchX,batchT in self.createMiniBatches(X_train,y_train,batchSize,self.K):\n",
    "                z,y = self.forward_propagation(self.w1, self.w2, batchX)\n",
    "                cost, dW1, dW2 = self.backward_propagation( batchX, batchT, z, y, self.w1, self.w2 , l)\n",
    "                self.w1 = self.w1 + lRate * dW1\n",
    "                self.w2 = self.w2 + lRate * dW2\n",
    "\n",
    "    \n",
    "    \n",
    "    def createMiniBatches(self , x, t,batchSize,K):\n",
    "        stacked = np.hstack((x, t ))\n",
    "        np.random.shuffle(stacked)  \n",
    "        batchesList = []\n",
    "        i=0\n",
    "        while i<stacked.shape[0]:\n",
    "            if (i+batchSize<stacked.shape[0]):\n",
    "                batchesList.append(stacked[i:i+batchSize])\n",
    "            else:\n",
    "                batchesList.append( stacked[i:])\n",
    "\n",
    "            i+=batchSize\n",
    "\n",
    "        batchesList = [ [ batch[:,:-K] , batch[:,-K:] ]  for batch in batchesList  ]\n",
    "        return batchesList\n",
    "    \n",
    "    \n",
    "    def calculate_accuracy(self,X_test, t_test):\n",
    "        X_test = np.hstack(( np.ones((X_test.shape[0],1)) , X_test ))\n",
    "        _ , y = self.forward_propagation(self.w1 ,self.w2 ,X_test )        \n",
    "        return np.mean( np.argmax(y,1) == np.argmax(t_test,1) )\n",
    "    \n",
    "    \n",
    "    def gradcheck(self,X,t, l):\n",
    "        w1 = np.zeros((self.M,X.shape[1]+1))\n",
    "        w2 = np.zeros((t.shape[1],self.M+1))   \n",
    "    \n",
    "        epsilon = 1e-6\n",
    "        X =  np.hstack(( np.ones((X.shape[0],1)) , X ))\n",
    "        _list = np.random.randint(X.shape[0], size=5)\n",
    "        x_sample = np.array(X[_list,:])\n",
    "        t_sample = np.array(t[_list,:])\n",
    "        old_W1 = np.copy(w1)\n",
    "        old_W2 = np.copy(w2)\n",
    "        \n",
    "        z,y = self.forward_propagation( w1 , w2,x_sample)\n",
    "        Ew, dW1 , dW2 = self.backward_propagation(x_sample, t_sample, z,y , w1 , w2,l)\n",
    "        num_dW1 = np.zeros(dW1.shape)\n",
    "        num_dW2 = np.zeros(dW2.shape)\n",
    "\n",
    "\n",
    "        # Compute all numerical gradient estimates and store them in\n",
    "        # the matrix numericalGrad\n",
    "        for k in range(num_dW1.shape[0]):\n",
    "            for d in range(num_dW1.shape[1]):\n",
    "                #add epsilon to the w[k,d]\n",
    "                w1 = np.copy(old_W1)\n",
    "                w1[k, d] += epsilon\n",
    "                z,y = self.forward_propagation( w1 , w2,x_sample)\n",
    "                e_plus, _ , _ = self.backward_propagation(x_sample, t_sample, z,y, w1 , w2,l)\n",
    "\n",
    "                #subtract epsilon to the w[k,d]\n",
    "                w1 = np.copy(old_W1)\n",
    "                w1[k, d] -= epsilon\n",
    "                z,y  = self.forward_propagation( w1 , w2,x_sample)\n",
    "                e_minus, _ , _ =self.backward_propagation(x_sample, t_sample, z,y , w1 , w2,l)\n",
    "\n",
    "                #approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "                num_dW1[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "                            \n",
    "        w1 = np.copy(old_W1)\n",
    "        z,y  = self.forward_propagation( w1 , w2,x_sample)\n",
    "        Ew, dW1 , dW2 = self.backward_propagation(x_sample, t_sample, z,y , w1 , w2,l)\n",
    "        for k in range(num_dW2.shape[0]):\n",
    "            for d in range(num_dW2.shape[1]):\n",
    "\n",
    "                #add epsilon to the w[k,d]\n",
    "                w2 = np.copy(old_W2)\n",
    "                w2[k, d] += epsilon\n",
    "                z,y  = self.forward_propagation( w1 , w2,x_sample)\n",
    "                e_plus, _ , _ = self.backward_propagation(x_sample, t_sample, z,y , w1 , w2,l)\n",
    "\n",
    "                #subtract epsilon to the w[k,d]\n",
    "                w2 = np.copy(old_W2)\n",
    "                w2[k, d] -= epsilon\n",
    "                z,y  = self.forward_propagation( w1 , w2,x_sample)\n",
    "                e_minus, _ , _ = self.backward_propagation(x_sample, t_sample, z,y , w1 , w2,l)\n",
    "\n",
    "                #approximate gradient ( E[ w[k,d] + theta ] - E[ w[k,d] - theta ] ) / 2*e\n",
    "                num_dW2[k, d] = (e_plus - e_minus) / (2 * epsilon)\n",
    "            \n",
    "\n",
    "        w2 = np.copy(old_W2)\n",
    "        return ( dW1, num_dW1 , dW2 , num_dW2 )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = load_mnist()\n",
    "X_train, X_test, y_train, y_test = load_data(\"data\",\"cifar\")\n",
    "        \n",
    "mlp = oneLayerMLP(100,\"softplus\")\n",
    "\n",
    "mlp.train(X_train, y_train, 0.001, 20, 128, 0.1)\n",
    "acc = mlp.calculate_accuracy(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_data(\"data\",\"mnist\")\n",
    "mlp = oneLayerMLP(100,\"softplus\")\n",
    "\n",
    "mlp.train(X_train, y_train, 0.001, 2, 128, 0.1)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = mlp.gradcheck(X_train, y_train,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between approximate and numerical gradient of W1:  6.720707704829195e-05\n",
      "Difference between approximate and numerical gradient of W2:  1.199390468858974e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference between approximate and numerical gradient of W1: \" , np.sum(np.abs(grads[0]-grads[1])) )\n",
    "print(\"Difference between approximate and numerical gradient of W2: \" , np.sum(np.abs(grads[2]-grads[3])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testHyperparameters():\n",
    "    cols = ['DataSet', 'Hidden Layers (M)', 'Activation Function','epochs','Learning rate' , 'Regularization' , \"Accuracy\" ]\n",
    "    datasets = [\"mnist\",\"cifar\"]\n",
    "    m_vals = [100,200,300]\n",
    "    activations = [\"softplus\" , \"tanh\" , \"cos\"]\n",
    "    epochs = [10,20]\n",
    "    rates = [0.01 , 0.001]\n",
    "    l_vals = [0.1,0.5]\n",
    "    rows=[]\n",
    "    i = 0\n",
    "    for dataset in datasets:\n",
    "        X_train, X_test, y_train, y_test = load_data(\"data\",dataset)\n",
    "        for m in m_vals:\n",
    "            for activation in activations:\n",
    "                for epoch in epochs:\n",
    "                    for rate in rates:\n",
    "                        for l in l_vals:\n",
    "                            \n",
    "                            mlp = oneLayerMLP(m,activation)\n",
    "                            mlp.train(X_train, y_train, rate, epoch, 128, l)\n",
    "                            acc = mlp.calculate_accuracy(X_test, y_test)\n",
    "                            rows.append([dataset,m,activation,epoch,rate,l,acc])\n",
    "                            i+=1\n",
    "                            print(i)\n",
    "                            \n",
    "    df = pd.DataFrame(rows,columns =cols)\n",
    "    df.to_csv(\"Results.csv\")\n",
    "    \n",
    "testHyperparameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
