{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax( x, ax=1 ):\n",
    "    m = np.max( x, axis=ax, keepdims=True )    #max per row\n",
    "    p = np.exp( x - m )\n",
    "    return ( p / np.sum(p,axis=ax,keepdims=True) )\n",
    "\n",
    "class oneLayerMLP:\n",
    "    \n",
    "    def __init__(self, hiddenUnits , activationFunction):\n",
    "        self.M = hiddenUnits\n",
    "        self.h , self.hDeriv = self.getFunctions(activationFunction)\n",
    "\n",
    "        \n",
    "    def getFunctions(self,activationName):\n",
    "        if activationName==\"softplus\":\n",
    "           return lambda x: np.log(1+np.exp(x)) , lambda x: 1/(1+np.exp(-x))\n",
    "        elif activationName==\"tanh\": \n",
    "            return lambda x: np.tanh(x) , lambda x: 1-np.power(np.tanh(x),2)\n",
    "        elif activationName==\"cos\":\n",
    "            return lambda x: np.cos(x) , lambda x: -np.sin(x)\n",
    "        else:\n",
    "            print(\"Wrong code\")\n",
    "            return lambda x: 0  \n",
    "        \n",
    "# \n",
    "        \n",
    "    def forward_propagation(self ,w1, w2, x):\n",
    "        z = self.h(x.dot(w1.T) )  \n",
    "        z = np.hstack(( np.ones((z.shape[0],1)) , z )) \n",
    "        y = softmax( z.dot(w2.T) )\n",
    "        return z, y\n",
    "\n",
    "    def backward_propagation( self, x, t, z, y, w1, w2 , l):\n",
    "        cost = np.sum(np.log(y)*t) - (l/2) * ( np.sum( np.square( w1 ) ) +  np.sum( np.square( w2 ) ) )    \n",
    "        error = t-y\n",
    "        dW1 = ( error.dot(w2[:,1:]) * self.hDeriv( x.dot(w1.T)) ).T.dot(x) - l*w1\n",
    "        dW2 = error.T.dot(z) - l*w2\n",
    "        return cost, dW1, dW2   \n",
    "    \n",
    "    \n",
    "    def train(self , X_train, y_train, lRate , train_epochs, batchSize, l):\n",
    "        X_train = np.append(np.ones((X_train.shape[0], 1)), X_train, axis=1)\n",
    "        lRate = lRate / batchSize\n",
    "        self.K = y_train.shape[1]\n",
    "        self.D = X_train.shape[1]\n",
    "         \n",
    "        s1 = np.sqrt(2/self.D)\n",
    "        s2 = np.sqrt(2/self.M + 1)       \n",
    "        self.w1 = np.random.uniform(-s1 ,s1  , (self.M,self.D)) \n",
    "        self.w2 = np.random.uniform(-s2 ,s2  , (self.K,self.M+1))    \n",
    "        \n",
    "        for e in range(train_epochs):\n",
    "            for batchX,batchT in self.createMiniBatches(X_train,y_train,batchSize):\n",
    "                z,y = self.forward_propagation(self.w1, self.w2, batchX)\n",
    "                cost, dW1, dW2 = self.backward_propagation( batchX, batchT, z, y, self.w1, self.w2 , l)\n",
    "                self.w1 = self.w1 + lRate * dW1\n",
    "                self.w2 = self.w2 + lRate * dW2\n",
    "\n",
    "    \n",
    "    \n",
    "    def createMiniBatches(self , x, t,batchSize):\n",
    "        K = 10\n",
    "        stacked = np.hstack((x, t ))\n",
    "        np.random.shuffle(stacked)  \n",
    "        batchesList = []\n",
    "        i=0\n",
    "        while i<stacked.shape[0]:\n",
    "            if (i+batchSize<stacked.shape[0]):\n",
    "                batchesList.append(stacked[i:i+batchSize])\n",
    "            else:\n",
    "                batchesList.append( stacked[i:])\n",
    "\n",
    "            i+=batchSize\n",
    "\n",
    "        batchesList = [ [ batch[:,:-K] , batch[:,-K:] ]  for batch in batchesList  ]\n",
    "        return batchesList\n",
    "    \n",
    "    \n",
    "    def calculate_accuracy(self,X_test, t_test):\n",
    "        X_test = np.hstack(( np.ones((X_test.shape[0],1)) , X_test ))\n",
    "        _ , y = self.forward_propagation(self.w1 ,self.w2 ,X_test )        \n",
    "        return np.mean( np.argmax(y,1) == np.argmax(t_test,1) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = load_mnist()\n",
    "X_train, X_test, y_train, y_test = load_cifar()\n",
    "        \n",
    "mlp = oneLayerMLP(100,\"softplus\")\n",
    "\n",
    "mlp.train(X_train, y_train, 0.001, 20, 100, 0.1)\n",
    "acc = mlp.calculate_accuracy(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4047"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dictionary = pickle.load(fo, encoding='bytes')\n",
    "    return dictionary\n",
    "\n",
    "def load_cifar():\n",
    "    X_train = []\n",
    "    y_train = np.empty((0,10), int)\n",
    "    for i in range(1,6):\n",
    "        currentFile = unpickle(\"data/cifar-10-batches-py/data_batch_\"+ str(i))\n",
    "        current_X = currentFile[b'data']\n",
    "        X_train.extend( current_X )\n",
    "        current_labels = np.array(currentFile[b'labels'])\n",
    "        current_y = np.squeeze(np.eye(10)[current_labels.reshape(-1)])\n",
    "        y_train = np.vstack((y_train, current_y))\n",
    "\n",
    "    testFile = unpickle(\"data/cifar-10-batches-py/test_batch\")\n",
    "    X_test = testFile[b'data']\n",
    "    labels = np.array(testFile[b'labels'])\n",
    "    y_test = np.squeeze(np.eye(10)[labels.reshape(-1)])\n",
    "\n",
    "    return np.array(X_train)/255, np.array(X_test)/255, y_train , y_test\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "\n",
    "    df = None\n",
    "    y_train = []\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/train%d.txt' % i, header=None, sep=\" \" )\n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(10) ] \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_train.append( hot_vector )   \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "    train_data = df.values\n",
    "    y_train = np.array( y_train )\n",
    "    \n",
    "    df = None\n",
    "    y_test = []\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/test%d.txt' % i, header=None, sep=\" \" )\n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_test.append( hot_vector )  \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "    test_data = df.values\n",
    "    y_test = np.array( y_test )\n",
    "\n",
    "    return train_data.astype(float)/255, test_data.astype(float)/255, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
