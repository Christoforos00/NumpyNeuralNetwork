{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the MNIST dataset. Reads the training and testing files and create matrices.\n",
    "    :Expected return:\n",
    "    train_data:the matrix with the training data\n",
    "    test_data: the matrix with the data that will be used for testing\n",
    "    y_train: the matrix consisting of one \n",
    "                        hot vectors on each row(ground truth for training)\n",
    "    y_test: the matrix consisting of one\n",
    "                        hot vectors on each row(ground truth for testing)\n",
    "    \"\"\"\n",
    "    \n",
    "    #load the train files\n",
    "    df = None\n",
    "    \n",
    "    y_train = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/train%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_train.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    train_data = df.values\n",
    "    y_train = np.array( y_train )\n",
    "    \n",
    "    #load test files\n",
    "    df = None\n",
    "    \n",
    "    y_test = []\n",
    "\n",
    "    for i in range( 10 ):\n",
    "        tmp = pd.read_csv( 'data/mnist/test%d.txt' % i, header=None, sep=\" \" )\n",
    "        #build labels - one hot vector\n",
    "        \n",
    "        hot_vector = [ 1 if j == i else 0 for j in range(0,10) ]\n",
    "        \n",
    "        for j in range( tmp.shape[0] ):\n",
    "            y_test.append( hot_vector )\n",
    "        #concatenate dataframes by rows    \n",
    "        if i == 0:\n",
    "            df = tmp\n",
    "        else:\n",
    "            df = pd.concat( [df, tmp] )\n",
    "\n",
    "    test_data = df.values\n",
    "    y_test = np.array( y_test )\n",
    "    \n",
    "    return train_data.astype(float)/255, test_data.astype(float)/255, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oneLayerMLP:\n",
    "    \n",
    "    def __init__(self, hiddenUnits , activationFunction):\n",
    "        self.M = hiddenUnits\n",
    "        self.h , self.hDeriv = self.getFunctions(activationFunction)\n",
    "        \n",
    "        \n",
    "    def getFunctions(self,activationName):\n",
    "        if activationName==\"softplus\":\n",
    "            return lambda x: np.log(1+np.exp(x)) , lambda x: 1/(1+np.exp(-x))\n",
    "        elif activationName==\"tanh\": \n",
    "            return lambda x: (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) , lambda x: 1 - ((np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)))**2        \n",
    "        elif activationName==\"cos\":\n",
    "            return lambda x: np.cos(x) , lambda x: -np.sin(x)       \n",
    "        else:\n",
    "            print(\"Wrong code\")\n",
    "            return lambda x: 0  \n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self,X,t,l,learningRate,batchSize,K,epochs):\n",
    "        X =  np.hstack(( np.ones((X.shape[0],1)) , X ))    #adding the bias unit\n",
    "        self.l = l\n",
    "        self.batchSize = batchSize\n",
    "        self.w1 = np.random.uniform( -np.sqrt(2/self.M) ,np.sqrt(2/self.M)  , (self.M,X.shape[1])) \n",
    "        self.w2 = np.random.uniform(-np.sqrt(2/K)  ,np.sqrt(2/K)  , (K,self.M+1)) \n",
    "        self.costs = []\n",
    "        \n",
    "        batchesList = self.createMiniBatches(X,t,batchSize,K)\n",
    "\n",
    "        for e in range(epochs):\n",
    "            epochCost = []\n",
    "            for batchX , batchT in batchesList:\n",
    "                y,z = self.forward_propagation( batchX )\n",
    "                cost , dW1 , dW2 = self.backward_propagation( batchX, batchT, z, y )\n",
    "                self.w1 =self.w1 + learningRate*dW1\n",
    "                self.w2 = self.w2 + learningRate*dW2\n",
    "                epochCost.append(cost)\n",
    "            self.costs.append( sum(epochCost)/self.batchSize )\n",
    "            \n",
    "            \n",
    "    def softmax(self, x, ax=1 ):\n",
    "        m = np.max( x, axis=ax, keepdims=True )        #max per row\n",
    "        p = np.exp( x - m )\n",
    "        return ( p / np.sum(p,axis=ax,keepdims=True) )\n",
    "    \n",
    "    \n",
    "    def forward_propagation(self,x):\n",
    "        z = self.h( x.dot(self.w1.T) )\n",
    "        z = np.hstack(( np.ones((z.shape[0],1)) , z )) \n",
    "        y = self.softmax( z.dot(self.w2.T) )\n",
    "        return y,z\n",
    "    \n",
    "    \n",
    "    def backward_propagation(self,x,t,z,y):\n",
    "        cost = np.sum(np.log(y)*t) - (self.l/2) * ( np.sum( np.square( self.w1 ) ) +  np.sum( np.square( self.w2 ) ) )    \n",
    "        error = t-y\n",
    "        dW1 = ( error.dot(self.w2[:,1:]) * self.hDeriv( x.dot(self.w1.T)) ).T.dot(x) - self.l*self.w1\n",
    "        dW2 = error.T.dot(z) - self.l*self.w2\n",
    "    \n",
    "        return cost, dW1, dW2\n",
    "    \n",
    "    \n",
    "    def createMiniBatches(self,x,t,batchSize,K):\n",
    "        stacked = np.hstack((x, t ))\n",
    "        np.random.shuffle(stacked)  \n",
    "        batchesList = []\n",
    "        i=0\n",
    "        while i<stacked.shape[0]:\n",
    "            if (i+batchSize<stacked.shape[0]):\n",
    "                batchesList.append(stacked[i:i+batchSize])\n",
    "            else:\n",
    "                batchesList.append( stacked[i:])\n",
    "\n",
    "            i+=batchSize\n",
    "\n",
    "        batchesList = [ [ batch[:,:-K] , batch[:,-K:] ]  for batch in batchesList  ]\n",
    "        return batchesList  \n",
    "    \n",
    "    \n",
    "    def calculate_accuracy(self,X_test, t_test):\n",
    "        X_test = np.hstack(( np.ones((X_test.shape[0],1)) , X_test ))\n",
    "        y , _ = self.forward_propagation(X_test )\n",
    "        \n",
    "        return np.mean( np.argmax(y,1) == np.argmax(t_test,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = oneLayerMLP(100,\"cos\")\n",
    "mlp.train(X_train,y_train,0.1,0.001,100,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.974"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.calculate_accuracy(X_test, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
